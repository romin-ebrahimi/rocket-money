import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


def coherence(
    embeddings: np.ndarray,
    labels: np.ndarray,
) -> float:
    """
    Args:
        embeddings:
        labels:

    Returns:
        (float) coherence score.
    """
    unique_labels = set(labels)
    coherence_scores = []

    for label in unique_labels:
        topic_embeddings = embeddings[labels == label]
        if len(topic_embeddings) > 1:
            sim_matrix = cosine_similarity(topic_embeddings)
            avg_coherence = np.mean(sim_matrix)
            coherence_scores.append(avg_coherence)

    return np.mean(coherence_scores)


def purity(probs: np.ndarray) -> float:
    """
    Calculate the purity metric from the probabilities generated by a BERTopic
    model. This metric checks if each transaction has a clear dominant topic.

    Args:
        probs: (np.array) of probabilities from a BERTopic model.

    Returns:
        (float) average purity metric.
    """
    # Maximum probability for each transaction.
    max_probability = np.max(probs, axis=1)
    # Average of the maximum probabilities.
    avg_purity = np.mean(max_probability)

    return avg_purity


def unclassified_rate(topics: list) -> float:
    """
    The BERTopic model outputs -1 for unclassified topics. This method
    calculates the percentage of topics that are -1 i.e. unclassified.

    Args:
        topics: (list) containing the classified topics from a BERTopic model.

    Returns:
        (float) percentage of topics that are unclassified.
    """
    rate = topics.count(-1) / len(topics)

    return rate
